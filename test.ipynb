{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GCNN\n",
    "import pkgutil\n",
    "from GCNN.kernels import CyclicGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = GCNN.gcnn.GroupEquivariantCNN(CyclicGroup(4), \n",
    "#                                 in_channels=1, \n",
    "#                                 out_channels=1, \n",
    "#                                 kernel_size=3, \n",
    "#                                 hidden_dims=[2,2],  \n",
    "#                                 resolution=(28,28), \n",
    "#                                 wavelet_type='b_spline', \n",
    "#                                 slices=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We demonstrate our models on the MNIST dataset.\n",
    "import torchvision\n",
    "import torch\n",
    "DATASET_PATH = './data'\n",
    "# We normalize the training data.\n",
    "train_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                                  ])\n",
    "\n",
    "# To demonstrate the generalization capabilities our rotation equivariant layers bring, we apply a random\n",
    "# rotation between 0 and 360 deg to the test set.\n",
    "test_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                 torchvision.transforms.RandomRotation(\n",
    "                                                     [0, 360],\n",
    "                                                     torchvision.transforms.InterpolationMode.BILINEAR,\n",
    "                                                     fill=0),\n",
    "                                                 torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                                 ])\n",
    "\n",
    "train_ds = torchvision.datasets.MNIST(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "test_ds = torchvision.datasets.MNIST(root=DATASET_PATH, train=False, transform=test_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images, labels in train_loader:\n",
    "#     print(images.shape)\n",
    "#     i = net(images)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class DataModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model_name - Name of the model/CNN to run. Used for creating the model (see function below)\n",
    "            model_hparams - Hyperparameters for the model, as dictionary.\n",
    "            optimizer_name - Name of the optimizer to use. Currently supported: Adam, SGD\n",
    "            optimizer_hparams - Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
    "        self.save_hyperparameters()\n",
    "        # Create model\n",
    "        self.model = create_model(model_name, model_hparams)\n",
    "        # Create loss module\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        return self.model(imgs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # AdamW is Adam with a correct implementation of weight decay (see here for details: https://arxiv.org/pdf/1711.05101.pdf)\n",
    "        optimizer = optim.AdamW(\n",
    "            self.parameters(), **self.hparams.optimizer_hparams)\n",
    "        return [optimizer], []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # \"batch\" is the output of the training data loader.\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss  # Return tensor to call \".backward\" on\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs).argmax(dim=-1)\n",
    "        acc = (labels == preds).float().mean()\n",
    "        # By default logs it per epoch (weighted average over batches)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs).argmax(dim=-1)\n",
    "        acc = (labels == preds).float().mean()\n",
    "        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "\n",
    "model_dict = {\n",
    "    'Cake_GCNN': GCNN.gcnn.Cake_GroupEquivariantCNN,\n",
    "    'GCNN': GCNN.gcnn.GroupEquivariantCNN\n",
    "}\n",
    "\n",
    "def create_model(model_name, model_hparams):\n",
    "    if model_name in model_dict:\n",
    "        return model_dict[model_name](**model_hparams)\n",
    "    else:\n",
    "        assert False, f\"Unknown model name \\\"{model_name}\\\". Available models are: {str(model_dict.keys())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "import os\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "CHECKPOINT_PATH = './checkpoints'\n",
    "\n",
    "def train_model(model_name, save_name=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
    "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
    "    \"\"\"\n",
    "    if save_name is None:\n",
    "        save_name = model_name\n",
    "\n",
    "    comet_logger = pl_loggers.CometLogger(save_dir=\"logs/\")\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),                          # Where to save models\n",
    "                         gpus=1 if str(device)==\"cuda:0\" else 0,                                             # We run on a single GPU (if possible)\n",
    "                         max_epochs=10,                                                                      # How many epochs to train for if no patience is set\n",
    "                        #  callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
    "                        #             LearningRateMonitor(\"epoch\")],\n",
    "                        #  logger=comet_logger,\n",
    "                        logger=None\n",
    "                        )\n",
    "    # trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
    "    \n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = DataModule.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(12) # To be reproducable\n",
    "        model = DataModule(model_name=model_name, **kwargs)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "        model = DataModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on test set\n",
    "    val_result = trainer.test(model.to(device), test_loader, verbose=False)\n",
    "    result = {\"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in offline mode\n",
      "c:\\Users\\Chase\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=0)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=0)` instead.\n",
      "  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 12\n",
      "c:\\Users\\Chase\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\Chase\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:601: UserWarning: Checkpoint directory checkpoints\\cnn-pretrained\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name        | Type                     | Params\n",
      "---------------------------------------------------------\n",
      "0 | model       | Cake_GroupEquivariantCNN | 38.2 K\n",
      "1 | loss_module | CrossEntropyLoss         | 0     \n",
      "---------------------------------------------------------\n",
      "38.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "38.2 K    Total params\n",
      "0.153     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ee2b5b602e4762a2f55ff8583960a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e74d35562d4a098917fa71e8ed399f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdb14e8906b4fec8b11c75f40a58660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6937f1a53324c3d93ede3ad2df0a436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1af2d32b0e41d8a1358e878c5d67fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2266b3c2634bcb823d3f85c00f760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6db8482766d42759c8b445284331b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a950fb1132463e96fe427b45393a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61311dcd94c42c1aa3112ba21ddd468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f95232e55544a2d8886dff96c23993a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2828e0445da947569809f0a126ce0697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55a4811c25f43e9b60d2204f3665ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c385b0952fa442359b49212d43a0aafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = train_model(  model_name=\"Cake_GCNN\",\n",
    "                    model_hparams={'group':CyclicGroup(4), \n",
    "                                'in_channels':1, \n",
    "                                'out_channels':10, \n",
    "                                'kernel_size':3, \n",
    "                                'hidden_dims':[32,16,16,16],  \n",
    "                                'resolution':(28,28), \n",
    "                                'wavelet_type':'b_spline', \n",
    "                                'slices':4},\n",
    "                    optimizer_name=\"Adam\",\n",
    "                    optimizer_hparams={\"lr\": 1e-2,\n",
    "                                        \"weight_decay\": 1e-4},\n",
    "                    save_name='cnn-pretrained')\n",
    "\n",
    "# gcnn_model, gcnn_results = train_model(model_name=\"GCNN\",\n",
    "#                                        model_hparams={\"in_channels\": 1,\n",
    "#                                                       \"out_channels\": 10,\n",
    "#                                                       \"kernel_size\": 5,\n",
    "#                                                       \"num_hidden\": 4,\n",
    "#                                                       \"hidden_channels\":16, # to account for the increase in trainable parameters due to the extra dimension in our feature maps, remove some hidden channels.\n",
    "#                                                       \"group\":CyclicGroup(order=4).to(device)},\n",
    "#                                        optimizer_name=\"Adam\",\n",
    "#                                        optimizer_hparams={\"lr\": 1e-2,\n",
    "#                                                           \"weight_decay\": 1e-4},\n",
    "#                                        save_name='gcnn-pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9b59d146d7f753e41be7772c003ed65c1ec96936bc0245500483b2174be48ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
